"""Quantum Variational Autoencoder (QVAE) implementation
for modeling high-dimensional multi-omic data.

This example demonstrates how a hybrid quantum-classical model can be used
for colon cancer progression studies. The code expects input in the same
format generated by `crcClean.py` and includes a small synthetic demo.
"""

# Standard imports
from __future__ import annotations
import argparse
from pathlib import Path
import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# PyTorch for classical components
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# PennyLane for quantum simulation
import pennylane as qml


# ---------------------------- Data utilities ---------------------------- #

def load_crc_csv(csv_path: Path) -> torch.Tensor:
    """Load multi-omic data saved by crcClean.py and return as tensor."""
    df = pd.read_csv(csv_path, index_col=0)
    data = df.to_numpy(dtype=np.float32)
    return torch.from_numpy(data)


def create_synthetic_data(samples: int = 50, features: int = 20) -> torch.Tensor:
    """Generate simple synthetic dataset for demonstration."""
    rng = np.random.default_rng(42)
    data = rng.normal(size=(samples, features)).astype(np.float32)
    return torch.from_numpy(data)


def simulate_butyrate(data: torch.Tensor, shift: float = 0.5) -> torch.Tensor:
    """Simulate effect of butyrate by shifting a subset of features."""
    perturbed = data.clone()
    if data.size(1) > 0:
        perturbed[:, 0] += shift  # toy example: shift first feature
    return perturbed


# ---------------------------- Quantum Layer ---------------------------- #

class QuantumLayer(nn.Module):
    """Variational quantum circuit embedded inside a PyTorch Module."""

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        outputs = []
        for sample in x:
            result = self.circuit(sample, self.weights)
            outputs.append(torch.as_tensor(result, dtype=torch.float32))  # Ensure float32
        return torch.stack(outputs)

    def __init__(self, n_qubits: int = 4, n_layers: int = 2):
        super().__init__()
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        # initialize trainable parameters for the circuit
        weight_shape = (n_layers, n_qubits)
        self.weights = nn.Parameter(torch.randn(*weight_shape) * 0.01)
        self.dev = qml.device("default.qubit", wires=n_qubits)

        @qml.qnode(self.dev, interface="torch")
        def circuit(inputs, weights):
            # Encode classical data into quantum state using angle encoding
            for i in range(n_qubits):
                qml.RY(inputs[i], wires=i)
            # Variational layers
            for l in range(n_layers):
                for i in range(n_qubits):
                    qml.RY(weights[l, i], wires=i)
                for i in range(n_qubits - 1):
                    qml.CZ(wires=[i, i + 1])
            return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]

        self.circuit = circuit

        #def forward(self, x: torch.Tensor) -> torch.Tensor:
            # Expect x with shape (batch, n_qubits)
            #outputs = []
            #for sample in x:
                #outputs.append(self.circuit(sample, self.weights))
            #return torch.stack(outputs)


# ---------------------------- QVAE Model ---------------------------- #

class QVAE(nn.Module):
    """Quantum Variational Autoencoder."""

    def __init__(self, input_dim: int, latent_dim: int, n_qubits: int = 4, n_layers: int = 2):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, n_qubits),
        )
        self.quantum = QuantumLayer(n_qubits, n_layers)
        self.decoder = nn.Sequential(
            nn.Linear(n_qubits, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, input_dim),
        )
        self.latent_dim = latent_dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        encoded = self.encoder(x)
        latent = self.quantum(encoded)
        decoded = self.decoder(latent)
        return decoded, latent


# ---------------------------- Training utilities ---------------------------- #

def train_qvae(model: QVAE, data_loader: DataLoader, epochs: int = 20, lr: float = 1e-3):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    criterion = nn.MSELoss()
    losses = []

    for epoch in range(epochs):
        total_loss = 0.0
        for batch in data_loader:
            x = batch[0]
            optimizer.zero_grad()
            x_hat, _ = model(x)
            loss = criterion(x_hat, x)
            loss.backward()
            optimizer.step()
            total_loss += loss.item() * x.size(0)
        avg_loss = total_loss / len(data_loader.dataset)
        losses.append(avg_loss)
        print(f"Epoch {epoch+1}/{epochs} - loss: {avg_loss:.4f}")
    return losses


# ---------------------------- Visualization ---------------------------- #

def plot_latent_space(latent: torch.Tensor, labels=None, title: str = "Latent space"):
    latent_np = latent.detach().cpu().numpy()
    embedding = latent_np
    if latent_np.shape[1] > 2:
        from sklearn.manifold import TSNE
        embedding = TSNE(n_components=2, perplexity=5, random_state=42).fit_transform(latent_np)
    plt.figure(figsize=(6, 5))
    if labels is not None:
        sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], hue=labels, palette="viridis")
    else:
        plt.scatter(embedding[:, 0], embedding[:, 1])
    plt.title(title)
    plt.xlabel("dim1")
    plt.ylabel("dim2")
    plt.tight_layout()
    plt.show()


# ---------------------------- Main demonstration ---------------------------- #

def main(args: argparse.Namespace):
    if args.csv.exists():
        data = load_crc_csv(args.csv)
    else:
        print("CSV not found, using synthetic data for demo.")
        data = create_synthetic_data(samples=40, features=args.features)

    dataset = TensorDataset(data)
    loader = DataLoader(dataset, batch_size=8, shuffle=True)

    model = QVAE(input_dim=data.size(1), latent_dim=args.qubits, n_qubits=args.qubits, n_layers=args.layers)
    train_qvae(model, loader, epochs=args.epochs, lr=args.lr)

    # Obtain latent representation
    with torch.no_grad():
        _, latent = model(data)
    plot_latent_space(latent, title="Latent space (baseline)")

    # Simulate dietary perturbation
    perturbed = simulate_butyrate(data, shift=0.7)
    with torch.no_grad():
        _, latent_p = model(perturbed)
    plot_latent_space(latent_p, title="Latent space (butyrate)")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="QVAE demo")
    parser.add_argument(
        "--csv",
        type=Path,
        default=Path(__file__).resolve().parent / "crc_consolidated.csv",
        help="Input data CSV",
    )
    parser.add_argument("--epochs", type=int, default=5)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--qubits", type=int, default=4, help="Number of qubits/latent dimensions")
    parser.add_argument("--layers", type=int, default=2, help="Number of quantum layers")
    parser.add_argument("--features", type=int, default=20, help="Synthetic feature count if csv missing")
    main(parser.parse_args())
